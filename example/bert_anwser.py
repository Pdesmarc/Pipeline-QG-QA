import argparse
from transformers import * 
import torch


def main():
    parser = argparse.ArgumentParser(
        description='Discover driving sessions into log files.')
    parser.add_argument('-t', '--text_anwser', help='full input text', required=True)
    parser.add_argument('-q', '--question_file', help='question generated', required=True)
    parser.add_argument('-o', '--output', help='Output folder', required=True)

    args = parser.parse_args()

    process(args.text_anwser,args.question_file,args.output)

def process(text_anwser, question_file, output):

  # SQuAD 1.1
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
  config = BertConfig.from_pretrained('bert-base-uncased')

  listParagraph = []
  listQuestion = []
  listTextProcessed = []

  with open(text_anwser, "r", encoding="utf-8") as fpar:
    paragraphs = [x.strip().split('[SEP]')[0] for x in fpar.readlines()]
    paragraphs = [x.lower() for x in paragraphs]

  with open(text_anwser, "r", encoding="utf-8") as fr:
    responses = [x.strip().split('[SEP]')[1] for x in fr.readlines()]
    responses = [x.lower() for x in responses]

  with open(question_file, "r", encoding="utf-8") as fq:
    questions = [x for x in fq.readlines()]
    questions = [x.lower() for x in questions]
  answers = []
  for j in range(len(questions)):
    ## Pour faire de l'inférence avec le checkpoint
    text = paragraphs[j]
    question = questions[j]
    #print("Number of tokens:", howManyTokens(text, question))
    #WordPiece tokenizer ==> real number of encoded tokens (512 max) + CLS, SEP and ##xx words
    input_ids = tokenizer.encode(question, text) #Pas inversible !!! 
    #print(torch.tensor([input_ids]).shape)
    #print(input_ids)

    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
    # Tous les tokens dont l'ID est inférieur à 102 correspond aux tokens de la question se trouvant donc avant [SEP]

    batch_size = 9
    from_seq_length = 384
    to_seq_length = 512
    #attention_mask = [batch_size, from_seq_length, to_seq_length]
    #atari = torch.tensor(np.arange(24).reshape(2,3,4))

    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))

    """   print("Token Type IDs: {}".format(token_type_ids))"""
    #print(len(start_scores))
    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)

    """print("TOKEN DEBUT MOST LIKELY:", input_ids[torch.argmax(start_scores)])
        print("")
        print("TOKEN DEBUT:", all_tokens[torch.argmax(start_scores)])"""

    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])
    answer = answer.replace(' ##', '')
    answers.append(answer)
    ##We need to detokenize the answer before printing it

    #assert answer == "nine"

    """for i, a in enumerate(answers):
    print("PARAGRAPH:",paragraphs[i])
    print("\n")
    print("QUESTION:",questions[i])
    print("\n")
    print("ANSWER PREDICTED:", a)
    print("GOLD ANSWER:", responses[i])
    print("\n")"""

  with open( output + '/resultat_final_scenario1.txt', "w+", encoding="utf-8") as fn_out:
    for i, a in enumerate(answers):      
      fn_out.write("PARAGRAPH: {}\n\n".format(paragraphs[i]))
      fn_out.write("ANSWER EXTRATECD BY SPACY: {}\n".format(responses[i]))
      fn_out.write("QUESTION GENERATED BY unilm: {}".format(questions[i]))
      fn_out.write("ANSWER PREDICTED BY Bert 1.1: {}\n\n\n".format(a))
      
    fn_out.close()

if __name__ == '__main__':
    main()